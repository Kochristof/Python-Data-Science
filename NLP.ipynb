{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLdF9AjRG7REady9YFChkK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kochristof/Python-Data-Science/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6ekocSaAPq0",
        "outputId": "41271b34-91f8-4a8e-995d-d4374049caf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txnCrovGATol",
        "outputId": "e4b09b7d-2ed4-430c-e3e9-a1e0206be2ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnukRlsa-5OW",
        "outputId": "70d24c7d-3b86-482d-a964-3b378ac356e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'awesome!']\n"
          ]
        }
      ],
      "source": [
        "# Defining a string variable containing the input text\n",
        "text = \"Natural Language Processing is awesome!\"\n",
        " # Splitting the text into tokens (words) based on whitespace\n",
        "tokens = text.split()\n",
        "# Printing the resulting list of tokens\n",
        "print(tokens)\n",
        "\n",
        "# Output:\n",
        "# ['Natural', 'Language', 'Processing', 'is', 'awesome!']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a string variable containing the input text\n",
        "text = input()\n",
        " # Splitting the text into tokens (words) based on whitespace\n",
        "tokens = text.split()\n",
        "# Printing the resulting list of tokens\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WsDWxf-BrUZ",
        "outputId": "4421d90c-57e6-4070-da0d-7c37f9d9bbf4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alasd\n",
            "['Alasd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        " # Sample text\n",
        "text = \"This is a sample sentence demonstrating stopword removal.\"\n",
        "\n",
        "# Creating a set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenizing the text into individual words\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "# Removing stopwords\n",
        "filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Printing the filtered text\n",
        "print(filtered_text)\n",
        "\n",
        "\n",
        "# Output: ['sample', 'sentence', 'demonstrating', 'stopword', 'removal', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdx85Qzg_8rz",
        "outputId": "0f4323f3-20b1-4c16-8b7a-9b299675a521"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', 'demonstrating', 'stopword', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the TfidfVectorizer class\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Defining a corpus of text documents\n",
        "corpus = input()\n",
        "\n",
        "# Split the input into a list of strings\n",
        "# This line is specific to the current snippet context and therefore may not appear elsewhere.\n",
        "corpus = corpus.split(',')\n",
        "\n",
        "# Initializing the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fitting the vectorizer to the corpus and transforming the corpus into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Printing the TF-IDF matrix in array format\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6hBfON-B0oB",
        "outputId": "9b734dd6-686a-4749-b453-04a42c5755c4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asddsa, asdas, asd \n",
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning a string to the variable 'text'\n",
        "text = \"NLP is Amazing!\"\n",
        "# Converting the text to lowercase\n",
        "normalized_text = text.lower()\n",
        "# Printing the normalized text\n",
        "print(normalized_text)\n",
        "\n",
        "\n",
        "# Output: 'nlp is amazing!'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG1mKYtKAJct",
        "outputId": "d8886c1f-064d-4a72-9e29-3f396a4ea2c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlp is amazing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning a string to the variable 'text'\n",
        "text = input()\n",
        "# Converting the text to lowercase\n",
        "normalized_text = text.lower()\n",
        "# Printing the normalized text\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV_5aTu3B_6f",
        "outputId": "79a6419e-c323-434f-e193-cf657506b3ba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asda \n",
            "asda \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Word to be processed\n",
        "word = \"running\"\n",
        "\n",
        "# Lemmatization\n",
        "lemma_word = lemmatizer.lemmatize(word)\n",
        "\n",
        "# Stemming\n",
        "stem_word = stemmer.stem(word)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"Lemmatized Word:\", lemma_word)\n",
        "print(\"Stemmed Word:\", stem_word)\n",
        "\n",
        "# Output:\n",
        "# Lemmatized Word: running\n",
        "# Stemmed Word: run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwtA9fO_BIot",
        "outputId": "b737563b-6d52-47c2-f2b7-a0ff20bdc431"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Word: running\n",
            "Stemmed Word: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Word to be processed\n",
        "word = input()\n",
        "\n",
        "# Lemmatization\n",
        "lemma_word = lemmatizer.lemmatize(word)\n",
        "\n",
        "# Stemming\n",
        "stem_word = stemmer.stem(word)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"Lemmatized Word:\", lemma_word)\n",
        "print(\"Stemmed Word:\", stem_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZTxp3mbCHzm",
        "outputId": "b7bfcb1b-12ea-4952-a5b0-321bdc609455"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sadas asd asdasd \n",
            "Lemmatized Word: sadas asd asdasd \n",
            "Stemmed Word: sadas asd asdasd \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# List of words\n",
        "words = [\"apple\", \"banana\", \"orange\", \"grape\"]\n",
        "\n",
        "# Create a dictionary to map each word to an index\n",
        "word_to_index = {word: i for i, word in enumerate(words)}\n",
        "\n",
        "# Function to perform one-hot encoding\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "    vector = np.zeros(len(word_to_index))\n",
        "    vector[word_to_index[word]] = 1\n",
        "    return vector\n",
        "\n",
        "# Example usage\n",
        "word = \"banana\"\n",
        "encoded_vector = one_hot_encoding(word, word_to_index)\n",
        "\n",
        "print(\"Word:\", word)\n",
        "print(\"One-Hot Encoded Vector:\", encoded_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us__h_joBTv4",
        "outputId": "0e335659-325d-4600-a1eb-1223357d0ddc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: banana\n",
            "One-Hot Encoded Vector: [0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# List of words\n",
        "words = [\"apple\", \"banana\", \"orange\", \"grape\"]\n",
        "\n",
        "# Create a dictionary to map each word to an index\n",
        "word_to_index = {word: i for i, word in enumerate(words)}\n",
        "\n",
        "# Function to perform one-hot encoding\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "    vector = np.zeros(len(word_to_index))\n",
        "    vector[word_to_index[word]] = 1\n",
        "    return vector\n",
        "\n",
        "# Example usage\n",
        "word = input()\n",
        "encoded_vector = one_hot_encoding(word, word_to_index)\n",
        "\n",
        "print(\"Word:\", word)\n",
        "print(\"One-Hot Encoded Vector:\", encoded_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SyWkk-eCEzP",
        "outputId": "483801f1-b3cd-4dd9-8b2f-d8e85245106b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orange\n",
            "Word: orange\n",
            "One-Hot Encoded Vector: [0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # Importing the ngrams function from NLTK\n",
        "\n",
        "# Defining a sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "# Splitting the sentence into words\n",
        "words = sentence.split()\n",
        "# Generating a list of bigrams (2-grams) from the words\n",
        "bigrams = list(ngrams(words, 2))\n",
        "# Printing the list of bigrams\n",
        "print(bigrams)\n",
        "\n",
        "\n",
        "# Output:\n",
        "#[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'),\n",
        "# ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDHzkboLBjl4",
        "outputId": "99a5a009-c63c-4c9c-d21d-ba90c1156f10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # Importing the ngrams function from NLTK\n",
        "\n",
        "# Defining a sample sentence\n",
        "sentence = input()\n",
        "# Splitting the sentence into words\n",
        "words = sentence.split()\n",
        "# Generating a list of bigrams (2-grams) from the words\n",
        "bigrams = list(ngrams(words, 2))\n",
        "# Printing the list of bigrams\n",
        "print(bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPZl4n4ECY_-",
        "outputId": "84fc4b3b-e20d-47d1-bb33-8e1206eb2b23"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aasda sadsdas asd asd asd \n",
            "[('aasda', 'sadsdas'), ('sadsdas', 'asd'), ('asd', 'asd'), ('asd', 'asd')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Counter class from the collections module\n",
        "from collections import Counter\n",
        "\n",
        "# Prompt the user to enter the text\n",
        "corpus = input()\n",
        "\n",
        "# Tokenize the input text into words\n",
        "input_words = corpus.split()\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = Counter(input_words)\n",
        "\n",
        "# Calculate word probabilities\n",
        "total_words = sum(word_freq.values())\n",
        "word_probs = {word: freq / total_words for word, freq in word_freq.items()}\n",
        "print(word_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3Ttmv4cDFZm",
        "outputId": "e2555075-8970-4968-8840-4d2090001f1d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asdasd asd as asd \n",
            "{'asdasd': 0.25, 'asd': 0.5, 'as': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the TfidfVectorizer class\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Defining a corpus of text documents\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# Initializing the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fitting the vectorizer to the corpus and transforming the corpus into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Printing the TF-IDF matrix in array format\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIHnyX1rDNOl",
        "outputId": "6d7e8f5e-7dbd-4ab6-e937-9101f0df495f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Importing the Counter class from the collections module\n",
        "from collections import Counter\n",
        "\n",
        "# Prompt the user to enter the text\n",
        "corpus = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = Counter(corpus)\n",
        "\n",
        "# Calculate word probabilities\n",
        "total_words = sum(word_freq.values())\n",
        "word_probs = {word: freq / total_words for word, freq in word_freq.items()}\n",
        "print(word_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uxYKg0ADhQc",
        "outputId": "1fcc6375-e1a9-4bbe-f5fd-27f4d884590c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 0.2222222222222222, 'quick': 0.1111111111111111, 'brown': 0.1111111111111111, 'fox': 0.1111111111111111, 'jumps': 0.1111111111111111, 'over': 0.1111111111111111, 'lazy': 0.1111111111111111, 'dog': 0.1111111111111111}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the spaCy library\n",
        "import spacy\n",
        "\n",
        "# Load the English NER model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the text containing named entities\n",
        "text = \"Apple is planning to build a new store in New York City.\"\n",
        "\n",
        "# Process the text with the NER model\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities from the processed text\n",
        "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "# Printing the extracted named entities and their labels\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNsY8U8xDkKN",
        "outputId": "79757cdb-76cf-4649-bc1d-c87c8d2c785f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('New York City', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the TextBlob class from the textblob library\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Define the text for sentiment analysis\n",
        "text = \"I love this product! It's amazing.\"\n",
        "\n",
        "# Create a TextBlob object with the input text\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis Calculating the sentiment polarity score (-1 to 1)\n",
        "sentiment_score = blob.sentiment.polarity\n",
        "\n",
        "# Classify sentiment based on the sentiment score\n",
        "if sentiment_score > 0:\n",
        "    sentiment = \"positive\"\n",
        "elif sentiment_score < 0:\n",
        "    sentiment = \"negative\"\n",
        "else:\n",
        "    sentiment = \"neutral\"\n",
        "\n",
        "\n",
        "# Print the sentiment classification\n",
        "print(\"Sentiment:\", sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fDNCRJ-FZb5",
        "outputId": "c2ae96e6-70e5-49c2-f7df-6aa673154704"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5pJIWfYF0cX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}